{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7006af8c-0ff5-440c-86d8-6079d4d363ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "72b23455-197a-4cca-9653-53664d0c48b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CUDA devices: 1\n",
      "CUDA Device 0: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Memory Usage - Allocated: 6834670592 bytes\n",
      "Memory Usage - Cached: 22779265024 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available CUDA devices\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(\"Number of CUDA devices:\", num_devices)\n",
    "\n",
    "    # Iterate over each CUDA device and print details\n",
    "    for i in range(num_devices):\n",
    "        device = torch.device(f\"cuda:{i}\")\n",
    "        print(f\"CUDA Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"Memory Usage - Allocated: {torch.cuda.memory_allocated(device)} bytes\")\n",
    "        print(f\"Memory Usage - Cached: {torch.cuda.memory_reserved(device)} bytes\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7599f",
   "metadata": {},
   "source": [
    "Explanation: Here we are checking for the availability of CUDA which is a parallel computing platform which is created by Nvidia.We are verifying if CUDA is available using Pytorch by the function torch.cuda.is_available(). If CUDA is available we are retreiving all the CUDA devices using the function torch.cuda.device_count()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b856902-0961-4f66-8047-001f2332bb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, T5Tokenizer, BertForSequenceClassification, T5ForConditionalGeneration\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "138155f6-5ee7-4044-a095-7377cc332e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV dataset\n",
    "df = pd.read_csv('C:/Users/geeth/Downloads/sample (1).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d32b47a3-ba37-4b91-9f2f-5e7d8418ab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id   ProductId          UserId                      ProfileName  \\\n",
      "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "5   6  B006K2ZZ7K   ADT0SRK1MGOEU                   Twoapennything   \n",
      "6   7  B006K2ZZ7K  A1SP2KVKFXXRU1                David C. Sullivan   \n",
      "7   8  B006K2ZZ7K  A3JRGQVEQN31IQ               Pamela G. Williams   \n",
      "8   9  B000E7L2R4  A1MZYO9TZK0BBI                         R. James   \n",
      "9  10  B00171APVA  A21BT40VZCCYT4                    Carol A. Reed   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                     1                       1      5  1303862400   \n",
      "1                     0                       0      1  1346976000   \n",
      "2                     1                       1      4  1219017600   \n",
      "3                     3                       3      2  1307923200   \n",
      "4                     0                       0      5  1350777600   \n",
      "5                     0                       0      4  1342051200   \n",
      "6                     0                       0      5  1340150400   \n",
      "7                     0                       0      5  1336003200   \n",
      "8                     1                       1      5  1322006400   \n",
      "9                     0                       0      5  1351209600   \n",
      "\n",
      "                                         Summary  \\\n",
      "0                          Good Quality Dog Food   \n",
      "1                              Not as Advertised   \n",
      "2                          \"Delight\" says it all   \n",
      "3                                 Cough Medicine   \n",
      "4                                    Great taffy   \n",
      "5                                     Nice Taffy   \n",
      "6  Great!  Just as good as the expensive brands!   \n",
      "7                         Wonderful, tasty taffy   \n",
      "8                                     Yay Barley   \n",
      "9                               Healthy Dog Food   \n",
      "\n",
      "                                                Text Sentiment  \n",
      "0  I have bought several of the Vitality canned d...  Positive  \n",
      "1  Product arrived labeled as Jumbo Salted Peanut...   neutral  \n",
      "2  This is a confection that has been around a fe...  Positive  \n",
      "3  If you are looking for the secret ingredient i...  Positive  \n",
      "4  Great taffy at a great price.  There was a wid...  Positive  \n",
      "5  I got a wild hair for taffy and ordered this f...  Positive  \n",
      "6  This saltwater taffy had great flavors and was...  Positive  \n",
      "7  This taffy is so good.  It is very soft and ch...  Positive  \n",
      "8  Right now I'm mostly just sprouting this so my...  negative  \n",
      "9  This is a very healthy dog food. Good for thei...   neutral  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "931c2ae1-5ffd-4731-abef-ebb6cd8cbc9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# check for null values\n",
    "df['Summary'].isnull().sum()  # no null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acdac9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id   ProductId          UserId                      ProfileName  \\\n",
      "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "5   6  B006K2ZZ7K   ADT0SRK1MGOEU                   Twoapennything   \n",
      "6   7  B006K2ZZ7K  A1SP2KVKFXXRU1                David C. Sullivan   \n",
      "7   8  B006K2ZZ7K  A3JRGQVEQN31IQ               Pamela G. Williams   \n",
      "8   9  B000E7L2R4  A1MZYO9TZK0BBI                         R. James   \n",
      "9  10  B00171APVA  A21BT40VZCCYT4                    Carol A. Reed   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                     1                       1      5  1303862400   \n",
      "1                     0                       0      1  1346976000   \n",
      "2                     1                       1      4  1219017600   \n",
      "3                     3                       3      2  1307923200   \n",
      "4                     0                       0      5  1350777600   \n",
      "5                     0                       0      4  1342051200   \n",
      "6                     0                       0      5  1340150400   \n",
      "7                     0                       0      5  1336003200   \n",
      "8                     1                       1      5  1322006400   \n",
      "9                     0                       0      5  1351209600   \n",
      "\n",
      "                                         Summary  \\\n",
      "0                          Good Quality Dog Food   \n",
      "1                              Not as Advertised   \n",
      "2                          \"Delight\" says it all   \n",
      "3                                 Cough Medicine   \n",
      "4                                    Great taffy   \n",
      "5                                     Nice Taffy   \n",
      "6  Great!  Just as good as the expensive brands!   \n",
      "7                         Wonderful, tasty taffy   \n",
      "8                                     Yay Barley   \n",
      "9                               Healthy Dog Food   \n",
      "\n",
      "                                                Text Sentiment  \n",
      "0  I have bought several of the Vitality canned d...  Positive  \n",
      "1  Product arrived labeled as Jumbo Salted Peanut...   neutral  \n",
      "2  This is a confection that has been around a fe...  Positive  \n",
      "3  If you are looking for the secret ingredient i...  Positive  \n",
      "4  Great taffy at a great price.  There was a wid...  Positive  \n",
      "5  I got a wild hair for taffy and ordered this f...  Positive  \n",
      "6  This saltwater taffy had great flavors and was...  Positive  \n",
      "7  This taffy is so good.  It is very soft and ch...  Positive  \n",
      "8  Right now I'm mostly just sprouting this so my...  negative  \n",
      "9  This is a very healthy dog food. Good for thei...   neutral  \n"
     ]
    }
   ],
   "source": [
    "# remove duplicates/ for every duplicate we will keep only one row of that type. \n",
    "df.drop_duplicates(subset=['Score','Summary'],keep='first',inplace=True) \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d8e74e0-602c-49ba-8340-69487657281c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizers\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2567d1",
   "metadata": {},
   "source": [
    "Explanation: Here we are initializing two different pre -trained tokenizers such as BERT and T5. Firstly we are initializing BERT as tokenizer by loading th pre-trained tokenizer from 'bert-base-uncased' model. Similarly we initialized T% tokenizer by loading the pre-trained tokenizer from 't5-small' model.Tokenizers are responsible for breaking down input into tokens that the models understand and process them effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23c1bff6-e78d-4f09-a605-586931b780f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "from transformers import BertModel\n",
    "\n",
    "# Model Initialization\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b0ef041-ff52-46ec-b545-36cfe505247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f2687db-1a81-4d48-adf0-a7036a6705f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, bert_tokenizer, t5_tokenizer):\n",
    "        self.data = dataframe\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.t5_tokenizer = t5_tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        review = self.data.iloc[idx]['Text']\n",
    "        summary = self.data.iloc[idx]['Summary']\n",
    "        \n",
    "        # Tokenize inputs for BERT\n",
    "        bert_inputs = self.bert_tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,  # Set max length to the desired maximum sequence length\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize inputs for T5\n",
    "        t5_inputs = self.t5_tokenizer(\n",
    "            \"summarize: \" + review,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512,  # Set max length to the desired maximum sequence length\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'bert_input_ids': bert_inputs['input_ids'].flatten(),\n",
    "            'bert_attention_mask': bert_inputs['attention_mask'].flatten(),\n",
    "            't5_input_ids': t5_inputs['input_ids'].flatten(),\n",
    "            't5_attention_mask': t5_inputs['attention_mask'].flatten(),\n",
    "            'summary': summary\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61623935",
   "metadata": {},
   "source": [
    "Explanation: Here we defined a custom dataset class named \"CustomDataset\" which we are inheriting from the Pytorch dataset class. This class is taking a dataframe containing the text data, along with BERT and T5 tokenizers as input.In the __getitem__ method, the review text and summary for each data sample are retrieved from the dataset. It then tokenizes the review text using both BERT and T5 tokenizers, with parameters for adding special tokens, truncation, and padding to guarantee consistent input lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5262fa5-59c1-4978-8ee0-f1e034bd2079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = CustomDataset(train_df, bert_tokenizer, t5_tokenizer)\n",
    "test_dataset = CustomDataset(test_df, bert_tokenizer, t5_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3cd07b38-dbc6-4a46-90dc-2afc21c69e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8bf0bf59-bc05-4a5f-8a52-a4e5e8bd0172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"device = \"+\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b618eab1-c382-4e7b-af5d-c4cb24884fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.to(device)\n",
    "t5_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d51aec66-b62c-4095-98d3-9bed6c66b8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.to(device)\n",
    "t5_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f1a7a73d-8728-425f-84db-b585aa644d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(list(bert_model.parameters()) + list(t5_model.parameters()), lr=2e-5)\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1e499a",
   "metadata": {},
   "source": [
    "Explanation: Here we are initializing an optimizer to train the neural network model using Adamw optimization algorithm. We are using two different parameters ie. bert_model and t5_model for training them together.The AdamW optimizer is instantiated with a learning rate of 2e-5 (0.00002). The number of epochs for training is set to 3, indicating that the entire dataset will be iterated over three times during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b1e78f09-aebf-4be5-b428-d99fbf68fe51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 200/200 [17:59<00:00,  5.40s/it]\n",
      "Epoch 2: 100%|██████████| 200/200 [17:05<00:00,  5.13s/it]\n",
      "Epoch 3: 100%|██████████| 200/200 [17:00<00:00,  5.10s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    bert_model.train()\n",
    "    t5_model.train()\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Epoch \" + str(epoch+1))):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        bert_input_ids = batch['bert_input_ids'].to(device)\n",
    "        bert_attention_mask = batch['bert_attention_mask'].to(device)\n",
    "        t5_input_ids = batch['t5_input_ids'].to(device)\n",
    "        t5_attention_mask = batch['t5_attention_mask'].to(device)\n",
    "\n",
    "        # Forward pass through BERT\n",
    "        bert_outputs = bert_model(input_ids=bert_input_ids, attention_mask=bert_attention_mask)\n",
    "        bert_last_hidden_state = bert_outputs.last_hidden_state\n",
    "        \n",
    "        # Forward pass through T5\n",
    "        t5_outputs = t5_model(input_ids=t5_input_ids, attention_mask=t5_attention_mask, labels=t5_input_ids)\n",
    "        t5_loss = t5_outputs.loss\n",
    "        \n",
    "        # Jointly optimize BERT and T5\n",
    "        loss = t5_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305577f",
   "metadata": {},
   "source": [
    "Explanation: This particular code depicts the training loop for fine-tuning two neural network models, bert_model and t5_model, which are most likely used for text summarization. It iterates over a set number of epochs, with each epoch including training on batches of data loaded from a train_loader. The optimizer's gradients are reset to zero once each epoch with optimizer.zero_grad(). The algorithm then makes forward runs through both models, gathering their outputs, most notably the latest hidden states for BERT and the loss for T5. The loss from T5 serves as the total loss for backpropagation. Backpropagation is performed using loss.backward(), which updates the parameters of both models concurrently using the optimizer's step() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "128cf431-fe13-439e-85e0-06d9c8281576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than most.\n",
      "Generated Summary: smells and looks good. best for finicky dogs.\n",
      "\n",
      "Review: Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as 'Jumbo'.\n",
      "Generated Summary: Product does not match description.\n",
      "\n",
      "Review: This is a confection that has been around a few centuries. It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar. And it is a tiny mouthful of heaven. Not too chewy, and very flavorful. I highly recommend this yummy treat. If you are familiar with the story of C.S. Lewis' 'The Lion, The Witch, and The Wardrobe' - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
      "Generated Summary: Delicious.\n",
      "\n",
      "Review: If you are looking for the secret ingredient in Robitussin I believe I have found it. I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda. The flavor is very medicinal.\n",
      "Generated Summary: Medicinal flavor, not for everyone.\n",
      "\n",
      "Review: Great taffy at a great price. There was a wide assortment of yummy taffy. Delivery was very quick. If you're a taffy lover, this is a deal.\n",
      "Generated Summary: Great variety, and excellent taste.\n",
      "\n",
      "Review: I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there was a bit too much red/black licorice-flavored pieces (just not my particular favorites). Between me, my kids, and my husband, this lasted only two weeks! I would recommend this brand of taffy -- it was a delightful treat.\n",
      "Generated Summary: tasty taffy, some flavors were bad.\n",
      "\n",
      "Review: This saltwater taffy had great flavors and was very soft and chewy. Each candy was individually wrapped well. None of the candies were stuck together, which did happen in the expensive version, Fralinger's. Would highly recommend this candy! I served it at a beach-themed party and everyone loved it!\n",
      "Generated Summary: perfect for parties.\n",
      "\n",
      "Review: This taffy is so good. It is very soft and chewy. The flavors are amazing. I would definitely recommend you buying it. Very satisfying!!\n",
      "Generated Summary: taffy was Soft, chewy, and flavorful.\n",
      "\n",
      "Review: Right now I'm mostly just sprouting this so my cats can eat the grass. They love it. I rotate it around with Wheatgrass and Rye too\n",
      "Generated Summary: Healthy option for pets.\n",
      "\n",
      "Review: This is a very healthy dog food. Good for their digestion. Also good for small puppies. My dog eats her required amount at every feeding.\n",
      "Generated Summary: Unique and flavorful hot sauce.\n",
      "\n",
      "Review: I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavor of this hot sauce makes it one of a kind! We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away! When we realized that we simply couldn't find it anywhere in our city we were bummed. Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it. If you love hot sauce..I mean really love hot sauce, but don't want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan. Just realize that once you taste it, you will never want to use any other sauce. Thank you for the personal, incredible service!\n",
      "Generated Summary: Recommended for weight management.\n",
      "\n",
      "Review: One of my boys needed to lose some weight and the other didn't. I put this food on the floor for the chubby guy, and the protein-rich, no by-product food up higher where only my skinny boy can jump. The higher food sits going stale. They both really go for this food. And my chubby boy has been losing about an ounce a week.\n",
      "Generated Summary: Not recommended.\n",
      "\n",
      "Review: My cats have been happily eating Felidae Platinum for more than two years. I just got a new bag and the shape of the food is different. They tried the new food when I first put it in their bowls and now the bowls sit full and the kitties will not touch the food. I've noticed similar reviews related to formula changes in the past. Unfortunately, I now need to find a new food that my cats will eat.\n",
      "Generated Summary: Fresh and delicious.\n",
      "\n",
      "Review: good flavor! these came securely packed... they were fresh and delicious! i love these Twizzlers!\n",
      "Generated Summary: Yummy and highly satisfying.\n",
      "\n",
      "Review: The Strawberry Twizzlers are my guilty pleasure - yummy. Six pounds will be around for a while with my son and I.\n",
      "Generated Summary: Great for snacking.\n",
      "\n",
      "Review: My daughter loves Twizzlers and this shipment of six pounds really hit the spot. It's exactly what you would expect...six packages of strawberry Twizzlers.\n",
      "Generated Summary: Satisfying purchase.\n",
      "\n",
      "Review: I love eating them and they are good for watching TV and looking at movies! It is not too sweet. I like to transfer them to a zip lock baggie so they stay fresh so I can take my time eating them.\n",
      "Generated Summary: Recommended only for candy lovers.\n",
      "\n",
      "Review: I am very satisfied with my Twizzler purchase. I shared these with others and we have all enjoyed them. I will definitely be ordering more.\n",
      "Generated Summary: Fresh and tasty.\n",
      "\n",
      "Review: Twizzlers, Strawberry my childhood favorite candy, made in Lancaster Pennsylvania by Y & S Candies, Inc. one of the oldest confectionery Firms in the United States, now a Subsidiary of the Hershey Company, the Company was established in 1845 as Young and Smylie, they also make Apple Licorice Twists, Green Color and Blue Raspberry Licorice Twists, I like them all I keep it in a dry cool place because is not recommended it to put it in the fridge. According to the Guinness Book of Records, the longest Licorice Twist ever made measured 1.200 Feet (370 M) and weighted 100 Pounds (45 Kg) and was made by Y & S Candies, Inc. This Record-Breaking Twist became a Guinness World Record on July 19, 1998. This Product is Kosher! Thank You Candy was delivered very fast and was purchased at a reasonable price. I was home bound and unable to get to a store so this was perfect for me.\n",
      "Generated Summary: Generous amounts, good value.\n",
      "\n",
      "Review: My husband is a Twizzlers addict. We've bought these many times from Amazon because we're government employees living overseas and can't get them in the country we are assigned to. They've always been fresh and tasty, packed well and arrive in a timely manner.\n",
      "Generated Summary: Consistent quality.\n",
      "\n",
      "Review: I bought these for my husband who is currently overseas. He loves these, and apparently his staff likes them also. There are generous amounts of Twizzlers in each 16-ounce bag, and this was well worth the price. Twizzlers, Strawberry, 16-Ounce Bags (Pack of 6)\n",
      "Generated Summary: Satisfying candy. Recommended for those with diabeties.\n",
      "\n",
      "Review: I can remember buying this candy as a kid and the quality hasn't dropped in all these years. Still a superb product you won't be disappointed with.\n",
      "Generated Summary: Recommended for oatmeal lovers.\n",
      "\n",
      "Review: I love this candy. After weight watchers I had to cut back but still have a craving for it.\n",
      "Generated Summary: Convenient and nutritious.\n",
      "\n",
      "Review: I have lived out of the US for over 7 yrs now, and I so miss my Twizzlers!! When I go back to visit or someone visits me, I always stock up. All I can say is YUM! Sell these in Mexico and you will have a faithful buyer, more often than I'm able to buy them right now. Product received is as advertised. Twizzlers, Strawberry, 16-Ounce Bags (Pack of 6)\n",
      "Generated Summary: Great texture and flavor.\n",
      "\n",
      "Review: The candy is just red , No flavor . Just plan and chewy . I would never buy them again\n",
      "Generated Summary: Convenient and tasty. Recommended for busy mornings.\n",
      "\n",
      "Review: I was so glad Amazon carried these batteries. I have a hard time finding them elsewhere because they are such a unique size. I need them for my garage door opener. Great deal for the price.\n",
      "Generated Summary: Delicious oatmeal. Highly recommended for oatmeal lovers.\n",
      "\n",
      "Review: I got this for my Mum who is not diabetic but needs to watch her sugar intake, and my father who simply chooses to limit unnecessary sugar intake - she's the one with the sweet tooth - they both LOVED these toffees, you would never guess that they're sugar-free and it's so great that you can eat them pretty much guilt free! i was so impressed that i've ordered some for myself (w dark chocolate) to take to the office so i'll eat them instead of snacking on sugary sweets. These are just EXCELLENT!\n",
      "Generated Summary: Tasty and convenient option.\n",
      "\n",
      "Review: I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind! We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away! When we realized that we simply couldn't find it anywhere in our city we were bummed. Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it. If you love hot sauce..I mean really love hot sauce, but don't want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan. Just realize that once you taste it, you will never want to use any other sauce. Thank you for the personal, incredible service!\n",
      "Generated Summary: Great taste and value.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model Testing\n",
    "bert_model.eval()\n",
    "t5_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        if i == 30:  # Stop after processing 10 batches\n",
    "            break\n",
    "        \n",
    "        bert_input_ids = batch['bert_input_ids'].to(device)\n",
    "        bert_attention_mask = batch['bert_attention_mask'].to(device)\n",
    "        t5_input_ids = batch['t5_input_ids'].to(device)\n",
    "        t5_attention_mask = batch['t5_attention_mask'].to(device)\n",
    "        \n",
    "        # Forward pass through BERT\n",
    "        bert_outputs = bert_model(input_ids=bert_input_ids, attention_mask=bert_attention_mask)\n",
    "        bert_last_hidden_state = bert_outputs.last_hidden_state\n",
    "        \n",
    "        # Forward pass through T5\n",
    "        t5_outputs = t5_model.generate(input_ids=t5_input_ids, attention_mask=t5_attention_mask, max_length=50, num_beams=4, early_stopping=True)\n",
    "        \n",
    "        # Decode the generated summaries\n",
    "        generated_summaries = t5_tokenizer.batch_decode(t5_outputs, skip_special_tokens=True)\n",
    "        \n",
    "        # Print the original review and generated summary\n",
    "         # Print the original review and generated summary\n",
    "        print(\"Review:\", test_dataset[i]['summary'])\n",
    "        print(\"Generated Summary:\", generated_summaries[0])  # Assuming batch size of 1\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ead306e",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "This particular code represents the testing step for trained models. It starts by putting both the BERT and T5 models in evaluation mode using eval() to suppress dropout and allow inference-specific behavior. Within a torch.no_grad() context, it iterates over batches of data from the test_loader, processing each batch in turn. It terminates processing after 30 batches by applying the condition if i == 30. For each batch, it sends the input data to the appropriate device for calculation.\n",
    "Then it does forward passes through both the BERT and T5 models to retrieve their results. T5 creates summaries using the generate technique, using options such as maximum length, number of beams, and early termination conditions. It then decodes the resulting summaries using the T5 tokenizer. Finally, it outputs both the original review and the produced summary, allowing for visual examination and evaluation of the model's ability to summarise material."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
